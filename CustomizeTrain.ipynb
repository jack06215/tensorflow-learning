{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Activation, LeakyReLU, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 50\n",
    "INPUT_DIM = 29\n",
    "LATENT_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Sequential(\n",
    "    [\n",
    "    InputLayer(input_shape=INPUT_DIM),\n",
    "    Dense(units=100, kernel_initializer='glorot_normal', activation='relu'),\n",
    "    Dense(units=50, kernel_initializer='glorot_normal', activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "    ], \n",
    "    name='descriminator'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Sequential(\n",
    "    [\n",
    "    InputLayer(input_shape=NOISE_DIM),\n",
    "    Dense(units=LATENT_DIM, kernel_initializer='glorot_normal'),\n",
    "    Dense(units=INPUT_DIM, activation='softmax'),\n",
    "    ],\n",
    "    name='generator'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"descriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 100)               3000      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 8,152\n",
      "Trainable params: 8,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 29)                2929      \n",
      "=================================================================\n",
      "Total params: 8,029\n",
      "Trainable params: 8,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f'{discriminator.summary()}\\n\\n{generator.summary()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(d_real, d_fake, metrics='JSD'):\n",
    "  if metrics in ['JSD', 'jsd']:\n",
    "      real_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(np.zeros(d_real.shape[0]),depth=2),logits=d_real))\n",
    "      fake_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(np.ones(d_fake.shape[0]),depth=2),logits=d_fake))\n",
    "      return real_loss + fake_loss\n",
    "  else:\n",
    "      raise ValueError\n",
    "\n",
    "def generator_loss(d_fake, metrics='JSD'):\n",
    "  if metrics in ['JSD', 'jsd']:\n",
    "      return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(np.ones(d_fake.shape[0]),depth=2),logits=d_fake))\n",
    "  else:\n",
    "      raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(Model):\n",
    "    def __init__(self, discriminator, generator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.noise_dim = NOISE_DIM\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(GAN, self).compile()\n",
    "        self.opt_d = d_optimizer\n",
    "        self.opt_g = g_optimizer\n",
    "    \n",
    "    def train_step(self, train_dataset):\n",
    "        if isinstance(train_dataset, tuple):\n",
    "            x_batch_train = train_dataset[0]\n",
    "        batch_size = tf.shape(x_batch_train)[0]\n",
    "\n",
    "        # Train discriminator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.noise_dim))\n",
    "        generated_data = self.generator(random_latent_vectors)\n",
    "        with tf.GradientTape() as tape:\n",
    "            d_real = self.discriminator(x_batch_train)\n",
    "            d_fake = self.discriminator(generated_data)\n",
    "            \n",
    "            loss_d = discriminator_loss(d_real, d_fake, 'JSD')\n",
    "        grads = tape.gradient(loss_d, self.discriminator.trainable_variables)\n",
    "        self.opt_d.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Train generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.noise_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            gz = self.generator(random_latent_vectors)\n",
    "            d_fake = self.discriminator(gz)\n",
    "            loss_g = generator_loss(d_fake, 'JSD')\n",
    "        grads = tape.gradient(loss_g, self.generator.trainable_variables)\n",
    "        self.opt_g.apply_gradients(\n",
    "            zip(grads, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"loss_d\": loss_d, \"loss_g\": loss_g}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(discriminator=discriminator, generator=generator)\n",
    "gan.compile(d_optimizer=Adam(learning_rate=0.0003), g_optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200000\n",
    "n_features = 29\n",
    "samples = make_classification(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(samples[0], samples[1], test_size=0.2, random_state=123)\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=512).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1440/1440 [==============================] - 3s 1ms/step - loss_d: 1.0409 - loss_g: 0.7189\n",
      "Epoch 2/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6947 - loss_g: 0.3812\n",
      "Epoch 3/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6303 - loss_g: 0.3171\n",
      "Epoch 4/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6275 - loss_g: 0.3142\n",
      "Epoch 5/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6268 - loss_g: 0.3136\n",
      "Epoch 6/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6266 - loss_g: 0.3134\n",
      "Epoch 7/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6266 - loss_g: 0.3133\n",
      "Epoch 8/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6265 - loss_g: 0.3133\n",
      "Epoch 9/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6265 - loss_g: 0.3133\n",
      "Epoch 10/10\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss_d: 0.6265 - loss_g: 0.3133\n"
     ]
    }
   ],
   "source": [
    "train_loss = gan.fit(train_dataset, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46c36fa438a0e5217cf9152cd05c2dbbe7da14e3af0a2f78612f95babe496324"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
