{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://neptune.ai/blog/keras-loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Activation, InputLayer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(10,))\n",
    "model.add(Dense(units=64, kernel_initializer='uniform'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_fn, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 704\n",
      "Trainable params: 704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy\n",
    "\n",
    "The Binary Cross entropy will calculate the cross-entropy loss between the predicted classes and the true classes. By default, the `sum_over_batch_size` reduction is used. This means that the loss will return the average of the per-sample losses in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83944494\n"
     ]
    }
   ],
   "source": [
    "y_true = [[0., 1.0], [0.2, 0.8],[0.3, 0.7],[0.4, 0.6]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6],[0.6, 0.4],[0.8, 0.2]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(reduction='sum_over_batch_size')\n",
    "print(bce(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum reduction means that the loss function will return the sum of the per-sample losses in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3577797\n"
     ]
    }
   ],
   "source": [
    "y_true = [[0., 1.0], [0.2, 0.8],[0.3, 0.7],[0.4, 0.6]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6],[0.6, 0.4],[0.8, 0.2]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(reduction='sum')\n",
    "print(bce(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the reduction as none returns the full array of the per-sample losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9162905  0.5919184  0.79465103 1.0549197 ]\n"
     ]
    }
   ],
   "source": [
    "y_true = [[0., 1.0], [0.2, 0.8],[0.3, 0.7],[0.4, 0.6]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6],[0.6, 0.4],[0.8, 0.2]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(reduction='none')\n",
    "print(bce(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Crossentropy\n",
    "\n",
    "The CategoricalCrossentropy also computes the cross-entropy loss between the true classes and predicted classes. The labels are given in an one-hot format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8131356\n"
     ]
    }
   ],
   "source": [
    "y_true = [[0.1, 1.0, 0.8], [0.1, 0.9, 0.1],[0.2, 0.7, 0.1],[0.3, 0.1, 0.6]]\n",
    "y_pred = [[0.6, 0.2, 0.2], [0.2, 0.6, 0.2],[0.7, 0.1, 0.2],[0.8, 0.1, 0.1]]\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "print(cce(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85900736\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 1, 2]\n",
    "y_pred = [[0.95, 0.05, 0], [0.1, 0.8, 0.1],[0.1, 0.8, 0.1]]\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "print(scce(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Poison Loss\n",
    "\n",
    "You can also use the Poisson class to compute the poison loss. It’s a great choice if your dataset comes from a Poisson distribution for example the number of calls a call center receives per hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9377117\n"
     ]
    }
   ],
   "source": [
    "y_true = [[0.1, 1.0, 0.8], [0.1, 0.9, 0.1],[0.2, 0.7, 0.1],[0.3, 0.1, 0.6]]\n",
    "y_pred = [[0.6, 0.2, 0.2], [0.2, 0.6, 0.2],[0.7, 0.1, 0.2],[0.8, 0.1, 0.1]]\n",
    "\n",
    "p = tf.keras.losses.Poisson()\n",
    "print(p(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence Loss\n",
    "\n",
    "The relative entropy can be computed using the KLDivergence class. According to the official docs at PyTorch:\n",
    "\n",
    "***KL divergence** is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1471658"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.1, 1.0, 0.8], [0.1, 0.9, 0.1],[0.2, 0.7, 0.1],[0.3, 0.1, 0.6]]\n",
    "y_pred = [[0.6, 0.2, 0.2], [0.2, 0.6, 0.2],[0.7, 0.1, 0.2],[0.8, 0.1, 0.1]]\n",
    "kl = tf.keras.losses.KLDivergence()\n",
    "kl(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Focal Loss\n",
    "\n",
    "In classification problems involving imbalanced data and object detection problems, you can use the Focal Loss. The loss introduces an adjustment to the cross-entropy criterion. \n",
    "\n",
    "It is done by altering its shape in a way that the loss allocated to well-classified examples is down-weighted. This ensures that the model is able to learn equally from minority and majority classes.\n",
    "\n",
    "The cross-entropy loss is scaled by scaling the factors decaying at zero as the confidence in the correct class increases. The factor of scaling down weights the contribution of unchallenging samples at training time and focuses on the challenging ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00010971 0.00329749 0.00030611]\n"
     ]
    }
   ],
   "source": [
    "y_true = [[0.97], [0.91], [0.03]]\n",
    "y_pred = [[1.0], [1.0], [0.0]]\n",
    "sfc = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "print(sfc(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Intersection over Union\n",
    "\n",
    "The Generalized Intersection over Union loss from the TensorFlow add on can also be used. The Intersection over Union (IoU) is a very common metric in object detection problems. IoU is however not very efficient in problems involving non-overlapping bounding boxes. \n",
    "\n",
    "The Generalized Intersection over Union was introduced to address this challenge that IoU is facing. It ensures that generalization is achieved by maintaining the scale-invariant property of IoU, encoding the shape properties of the compared objects into the region property, and making sure that there is a strong correlation with IoU in the event of overlapping objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.5041667, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gl = tfa.losses.GIoULoss()\n",
    "boxes1 = [[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]]\n",
    "boxes2 = [[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]]\n",
    "\n",
    "loss = gl(boxes1, boxes2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "The MeanSquaredError class can be used to compute the mean square of errors between the predictions and the true values.\n",
    "\n",
    "Use Mean Squared Error when you desire to have large errors penalized more than smaller ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.25\n"
     ]
    }
   ],
   "source": [
    "y_true = [12.0, 20.0, 29.0, 60.0]\n",
    "y_pred = [14.0, 18.0, 27.0, 55.0]\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "print(mse(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error\n",
    "\n",
    "The mean absolute percentage error is computed using the function below.\n",
    "\n",
    "$$loss = 100 \\cdot \\frac{|y_{true} - y_{pred}|} {y_{true}} $$\n",
    "\n",
    "It is calculated as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.474138\n"
     ]
    }
   ],
   "source": [
    "y_true = [12.0, 20.0, 29.0, 60.0]\n",
    "y_pred = [14.0, 18.0, 27.0, 55.0]\n",
    "mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "print(mape(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Logarithmic Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010642167\n"
     ]
    }
   ],
   "source": [
    "y_true = [12.0, 20.0, 29.0, 60.0]\n",
    "y_pred = [14.0, 18.0, 27.0, 55.0]\n",
    "msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "print(msle(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Loss\n",
    "\n",
    "If your interest is in computing the cosine similarity between the true and predicted values, you’d use the CosineSimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9963575"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[12.0, 20.0], [29.0, 60.0]]\n",
    "y_pred = [[14.0, 18.0], [27.0, 55.0]]\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "print(cosine_loss(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogCosh Loss\n",
    "\n",
    "The LogCosh class computes the logarithm of the hyperbolic cosine of the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0704765"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[12.0, 20.0], [29.0, 60.0]]\n",
    "y_pred = [[14.0, 18.0], [27.0, 55.0]]\n",
    "\n",
    "l = tf.keras.losses.LogCosh()\n",
    "print(l(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber loss\n",
    "\n",
    "For regression problems that are less sensitive to outliers, the Huber loss is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.25\n"
     ]
    }
   ],
   "source": [
    "y_true = [12.0, 20.0, 29.0, 60.0]\n",
    "y_pred = [14.0, 18.0, 27.0, 55.0]\n",
    "\n",
    "h = tf.keras.losses.Huber()\n",
    "print(h(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom loss functions in Keras\n",
    "\n",
    "A custom loss function can be created by defining a function that takes the true values and predicted values as required parameters. The function should return an array of losses. The function can then be passed at the compile stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "   squared_difference = tf.square(y_true - y_pred)\n",
    "   return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.25\n"
     ]
    }
   ],
   "source": [
    "y_true = [12.0, 20.0, 29.0, 60.0]\n",
    "y_pred = [14.0, 18.0, 27.0, 55.0]\n",
    "\n",
    "cl = custom_loss_function(np.array(y_true), np.array(y_pred))\n",
    "print(cl.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Keras loss weights\n",
    "\n",
    "During the training process, one can weigh the loss function by observations or samples. The weights can be arbitrary but a typical choice are class weights (distribution of labels). Each observation is weighted by the fraction of the class it belongs to (reversed) so that the loss for minority class observations is more important when calculating the loss.  \n",
    "\n",
    "One of the ways for doing this is passing the class weights during the training process. \n",
    "\n",
    "The weights are passed using a dictionary that contains the weight for each class. You can compute the weights using Scikit-learn or calculate the weights based on your own criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = { \n",
    " 0: 1.01300017,\n",
    " 1: 0.88994364,\n",
    " 2: 1.00704935,\n",
    " 3: 0.97863318,\n",
    " 4: 1.02704553,\n",
    " 5: 1.10680686,\n",
    " 6: 1.01385603,\n",
    " 7: 0.95770152,\n",
    " 8: 1.02546573,\n",
    " 9: 1.00857287\n",
    "}\n",
    "# model.fit(x_train, y_train, verbose=1, epochs=10,class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is to pass these weights at the compile stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1.013, 0.889, 1.007, 0.978, 1.027,1.106,1.013,0.957,1.025, 1.008]\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              loss_weights=weights,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46c36fa438a0e5217cf9152cd05c2dbbe7da14e3af0a2f78612f95babe496324"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
